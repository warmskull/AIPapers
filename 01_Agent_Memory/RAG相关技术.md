# 这个文档记录我对RAG相关的技术的学习笔记 


## 需要下载的论文


## 一篇Deep Research的结果

2025-2026 年 Retrieval-Augmented Generation (RAG) 技术最新进展与实践报告
最新学术研究成果综述
新模型架构的扩展：End-to-End RAG 与 Multi-hop RAG

近期的研究致力于将检索增强生成无缝融入模型架构，实现端到端的优化。例如，有研究提出通过联合优化检索器和生成器来提升性能，如 JSA-RAG 方法。JSA-RAG 使用联合随机近似（Joint Stochastic Approximation）算法解决了以往端到端训练中梯度估计方差大的问题，在开放域问答和知识对话等任务上明显优于传统的独立训练方式和变分RAG（VRAG）。同时，也有研究尝试通过反馈式奖励优化训练RAG模型，例如 MM-RewardRAG，该方法通过引入可验证的奖励模型，将全局奖励从答案质量反传到检索和生成组件，实现多模态RAG的端到端优化。MM-RewardRAG 在多模态问答基准上取得了最新的性能，用仅约4k条无标注问题的弱监督，达到了在文本+图像混合知识上的SOTA表现。

与此同时，为了应对复杂问答需要跨越多个证据的情况，多跳 (multi-hop) RAG 成为了研究热点。传统的 RAG 管线虽能检索并生成答案，但在涉及多步推理时容易出现检索偏差和上下文丢失。2025年前后出现了一批增强多跳推理能力的RAG模型。例如 RT-RAG (Reasoning Tree-guided RAG) 使用显式的问题分解树，通过共识驱动的树结构将复杂问题拆解，再自底向上逐步检索证据并融合答案，显著减少了推理链路中的误差传播。RT-RAG 在HotpotQA等多跳问答数据集上比现有方法平均提升了约7% F1、6% EM，体现了此结构化方法在复杂推理场景中的优势。另一项ACL 2025 工作 HopRAG 则引入逻辑感知的图检索机制：它在索引阶段通过生成伪查询构建段落知识图，查询时执行“检索-推理-裁剪”循环，结合LLM推理跳转相关节点。HopRAG 利用这种图式多跳检索，在MuSiQue、HotpotQA等数据集上对比传统RAG取得了大幅精度提升，答案准确率相对密集向量检索基线提高约76.8%，检索F1提高约65%，甚至超越其它结构化RAG如 RAPTOR 等约10%的准确率。这些研究表明，将逻辑推理与检索紧密结合（如通过树结构或图结构）是提高多跳RAG性能的有效途径。

多模态 RAG、多语言 RAG、时间感知 RAG 的新进展

在文本以外，多模态 RAG 近年获得进展，以应对跨文本、图像等多种数据源的查询场景。例如 2025 年一项研究提出 AlzheimerRAG，将 PubMed 文献中的图像和文本结合用于老年痴呆案例问答。该系统通过跨模态注意力融合技术，将文本和医学图像共同编码，并使用高效索引访问海量生物医学知识。实验显示，与 BioASQ、PubMedQA 等医学问答基准相比，多模态RAG能显著改善领域信息的检索和综合效果，生成的回答准确度可与人工相当且幻觉率低。此外，另一项 2025 年 EMNLP 工作 MM-RewardRAG 针对视觉问答，将文本知识库与图像知识库同时纳入检索，使用奖励信号端到端调优，成功在多个需要文字+图像推理的任务上取得最佳结果。这些都展现了多模态RAG在医疗、产品说明等领域的应用潜力。

在多语言 RAG 方面，随着LLM应用于非英文环境，如何跨语言检索成为关键课题。2024 年 NAVER 的研究构建了涵盖13种语言的 mRAG（multilingual RAG） 管线。他们发现：即使具备强大的多语种检索模型和多语种生成模型，仍需特别的 Prompt 工程来确保生成器用用户提问对应的语言给出答案。例如，为英文LLM编写提示，明确要求其用法语作答，以避免回答语言错乱。此外，现有评估指标对多语言也需调整，比如针对专有名词不同语言拼写、翻译差异的问题。这项研究还公开了多语言RAG实现代码，为后续工作提供了基线。跨语言检索的难点在于：检索器需能理解多语查询并在多语料库中找到相关内容。因此，一些工作采用对比学习训练通用跨语言向量表示，使不同语言的相似语义文本彼此接近，进而实现语言无关的检索。这在多语言问答、跨语言客服等场景具有重要意义。

时间感知（时序）RAG 是另一前沿方向。传统RAG多假设知识库静态不变，但现实中知识随时间演化，模型需具备“时序意识”才能回答时效性强的问题。为此，2025 年有研究提出 Temporal Graph RAG (TG-RAG)，用双层时间图来表示知识。底层是附有时间戳的知识图谱，将事实按发生时间区分节点和边；上层是按时间组织的分层时间树，汇总各时间点的重要事件。当新文档到来时，系统可将其中新事实插入时间图，仅为新增的时间节点生成摘要并更新其祖先节点，从而高效增量更新知识库。在推理时，TG-RAG 根据查询的时间范围动态检索相应子图，获取精准证据。该工作还构建了一个评估增量更新能力的时序问答数据集 ECT-QA，内容涵盖公司财报等具有年份演化的数据。实验结果显示，TG-RAG 在处理时间敏感知识、支持知识更新方面大幅领先于传统基线。这证明通过显式的时间建模（如将2021年和2022年的事实区分开来）可以让RAG在回答**“何时”**相关的问题时更加准确。类似地，其他工作如 TimeRAG、TD-RAG 等也探索了检索时限定时间窗口、基于时间戳索引等策略。总体而言，时间感知RAG能够让LLM紧跟最新信息并避免因知识时效性导致的错误，对构建实时问答和新闻分析系统具有重大意义。

检索器与生成器的联合训练技术

传统RAG多采用独立训练的检索模型（如BM25或DPR）和生成模型，然后简单串联。但近年来研究者尝试联合训练检索器与生成器，使二者协同优化、互相提升性能。一种思路是通过端到端反向传播来更新检索组件。例如JSA-RAG前面提到的联合随机近似方法，解决了检索分数离散导致的梯度高方差难题，使检索器参数可以随生成损失一起优化。实验表明，与仅微调生成器相比，联合训练后的检索-生成组合在答案的正确性和对检索证据的利用率上都有明显提高。另一种思路是强化学习或反馈学习：通过设计算法，让生成器产出答案后，由判别器或人反馈评估答案的正确性，将该评价作为奖励信号来更新检索器。例如前述 MM-RewardRAG 利用一个可验证的奖励模型比较生成答案与真实答案，根据匹配程度生成奖励，将其分别传播给重排序器和查询翻译器，调整它们的参数。这种带有RL风格的训练使检索模块学会挑选那些对最终回答有贡献的证据，即使初始检索可能包含间接相关的信息，也能通过多跳推理将有用线索串联起来。此外，也有工作采用变分推断(Variational)思想来训练RAG，例如 VRAG 尝试用变分下界近似联合概率，但存在梯度偏差或方差大问题，被JSA-RAG等改进方法解决。总的来看，联合训练可以让检索器更贴合生成器的需求，减少pipeline式独立优化带来的不匹配，进而缓解“检索到但未用好”或“模型幻觉引入非检索内容”等问题。随着研究推进，我们有望看到更多稳定高效的端到端RAG训练范式，真正实现“检索-生成一体化”的大模型。

RAG 性能评估的新指标与基准数据集

RAG系统的评估由于涉及检索和生成两个部分而较为复杂。2024年一篇综述指出，相关性、准确性、事实一致性等都是需要关注的量化指标。然而，以往基准往往侧重某一方面，缺乏对RAG全链条的细粒度评测。因此，近期出现了专门为RAG设计的评测框架和数据集。例如 2025 年 NAACL 提出的 MIRAGE 基准。MIRAGE 包含7,560个精标问答实例和一个包含37,800条知识的检索库，可高效评测系统的检索和生成两个环节。特别地，它引入了针对RAG系统适应性的新指标：

噪声脆弱性 (Noise Vulnerability)：衡量模型对检索到的无关信息的鲁棒性，如果检索结果里混入干扰项，模型输出正确性的下降程度。

上下文接受度 (Context Acceptability)：评估模型是否有效地利用了检索内容回答问题（避免无视提供的证据）。

上下文不敏感度 (Context Insensitivity)：检查模型回答是否对替换或移除部分检索内容过于不敏感（理想情况下，关键证据变化应影响答案）。

上下文误解率 (Context Misinterpretation)：考察模型是否曲解了检索到的内容含义而产生错误结论。

通过这些维度，MIRAGE 可全面诊断RAG系统在利用外部知识方面的能力和不足。实验还揭示，不同检索器-LLM组合在这些指标上有显著差异，为模型选择提供了新视角。

同时，2024 年的一个评价 Survey 强调需要统一的评估流程（AUEPORA），比较不同数据集和指标，发现当前基准的局限并指明改进方向。例如，大规模Wiki数据虽常用于检索评测，但对时效性、领域覆盖并不充分。一些新数据集如 KELVIN、实时新闻QA 等也被提出，以测试RAG在最新知识或特定领域上的表现。这些评测工作对于RAG至关重要：它们驱动着研究人员关注幻觉率（生成内容不在检索证据中）、知识利用率（检索内容在答案中体现的比例）等以前难以量化的方面。未来我们可能会看到更加标准化的RAG评测平台，将检索质量（如Recall@K、nDCG）和生成质量（如事实准确率、BLEU/ROUGE、LLM判分）结合起来，全面推动RAG系统走向成熟。

最新工程实践方法
开源框架与库支持

构建 RAG 系统的工程实践已经有丰富的开源生态支持。HuggingFace Transformers 库自2020年就集成了RAG模型模块。Transformers提供了 RagRetriever 和 RagSequenceForGeneration 等高级API，可以直接加载诸如 facebook/rag-sequence-nq 这样的预训练RAG模型和DPR检索器。这让开发者只需提供自定义的知识索引（或使用内置的Wikipedia DPR索引），就能实现“查询->检索->生成”的端到端推理。利用Transformers的RAG接口，可以通过替换底层检索索引来无缝更新知识，而不必重新训练生成模型。除了Transformers，HuggingFace还在教程和案例层面支持RAG，例如提供了结合LangChain和自家文档的高级RAG Notebook示例，演示如何调优各组件以提升性能。

LangChain 是近年来兴起的用于LLM应用编排的框架，对RAG也有完善支持。LangChain封装了向量存储接口、检索器和LLM调用，使开发者可以方便地构建“文档加载-切分-索引-检索-生成”流水线。官方文档中有专门的教程演示如何用LangChain构建RAG问答代理：从索引网页数据，到定义retrieve_context工具检索向量库，再通过Agent将检索结果加入LLM提示完成问答。LangChain支持多种向量数据库和嵌入模型的集成，并提供RAG Chain和RAG Agent两种模式以适配不同需求（一次LLM调用的快速解答 vs. 多步Tool调用的复杂推理）。由于LangChain的模块化设计，工程师可以很方便地替换其中的检索器、嵌入模型或LLM，从而测试不同组合的效果。

另一重要的开源项目是 LlamaIndex (原 GPT Index)。它提供了灵活的检索-生成管道构建工具，尤其擅长索引结构的自定义。LlamaIndex允许开发者将文档索引构造成树、图、向量等多种形式，并可以应用后处理（如Metadata过滤、自定义检索Node合并）来提升效果。例如，LlamaIndex可以方便地实现分层检索：先粗略筛选相关主题文档，再在细粒度段落上精检索。它还支持多模态索引和知识图谱扩展，适合需要复杂数据处理的RAG场景。实际上，LlamaIndex官方文档中也有使用该框架实现RAG的教程（如“Agent + RAG”的系列），展示如何一步步添加Embedding模型、构建VectorStore索引、封装QueryEngine供Agent使用。总的来说，LangChain 偏重于流程编排与工具集成，而 LlamaIndex 偏重于索引及数据结构，两者常常可以结合使用，在实际项目中发挥各自所长。

值得一提的还有 Haystack（deepset开发）等框架，虽然在问题中未提及，但也被广泛用于RAG应用。Haystack提供端到端解决方案，包括文档预处理、索引（支持Elasticsearch/FAISS等）、检索、Reader读取器（可用Transformers模型）等组件，初学者用它很快就能搭建一个问答系统。随着生态发展，如今还有LangChain-Haystack集成，使用户可以同时利用两者的能力。

向量数据库的选择与部署

RAG 系统的检索效率高度依赖于底层的向量检索库/数据库。题中提到的 FAISS、Milvus、Weaviate 都是业界常用的方案，各有特点：

FAISS：Facebook AI Research 开发的相似度搜索库，是事实上的标准。FAISS 并非完整的数据库，而更像底层的向量索引引擎。它提供多种索引算法（如Flat、IVF、HNSW、PQ等）以在不同内存/精度要求下取得最佳性能。FAISS 支持CPU和GPU加速，大数据集在GPU上搜索非常快。它还能处理超过内存大小的数据集（通过基于IVF等索引将部分数据存盘）。很多开源项目（包括Transformers的RagRetriever默认实现）内部都调用FAISS。对于原型开发或小型应用，可以直接用FAISS构建本地向量索引来支撑RAG。但FAISS缺少分布式和高级特性，所以在大规模部署时常与其它工具组合使用。

Milvus：一款开源的分布式向量数据库，专为海量向量存储和近实时检索设计。Milvus 原生支持横向扩展，可在多节点上管理数十亿向量的数据集。它针对相似度搜索做了各种优化（比如基于HNSW、IVF_PQ等索引实现低延迟查询），并支持结合TensorRT等进行GPU加速。Milvus 提供类似SQL的查询接口和丰富的SDK，易于和应用集成。同时其社区活跃、文档完善，成为构建生产级RAG向量库的常见选择之一。比如很多企业级文档问答系统会用Milvus做后端，以保证扩展性和可靠性。

Weaviate：另一个流行的开源向量数据库，特色是在向量检索之上支持混合搜索和知识图谱式的schema。Weaviate允许为数据对象定义属性并存储，这意味着你可以对元数据过滤，例如“只检索标签为X的文档”。它的查询接口支持GraphQL，非常适合构建带条件的语义搜索。Weaviate 同样支持集群扩展和HNSW等高效索引，并提供多语言客户端。它强调多租户隔离和企业安全，有内置的多租用户管理和数据隔离机制。因此在一些对数据安全要求高的场景（如云服务提供RAG能力）下，Weaviate 是理想选择。此外，Weaviate还能通过模块插件支持跨模态搜索（例如同时存文本向量和图像向量）。在RAG实践中，如果需要对索引内容按类别、时间等筛选，或需要方便地结合结构化查询，Weaviate会比纯粹的向量引擎更合适。

除了上述三者，还有 Pinecone、Chroma、Qdrant、Elasticsearch (向量模式) 等也常用于RAG后端。Pinecone是云托管服务，省去了运维，适合快速上线；Chroma是简洁的本地向量DB，易用但目前主要单机；Qdrant与Milvus类似也是Rust实现的高性能向量库。最新的比较显示，这些数据库各有优势：如 Pinecone 易用扩展强但非开源，Chroma 集成LangChain紧密适合中小数据集，Qdrant在Rust社区有良好性能等等。工程上应该根据应用规模、延迟要求和团队偏好进行选择。如仅几百万向量以内的数据，FAISS/Chroma足矣；上亿向量则考虑Milvus/Weaviate/Qdrant等分布式方案。

部署向量数据库也有一些最佳实践：持久化存储很关键，要避免容器重启导致数据丢失，需使用挂载卷或云盘存储索引文件。在Kubernetes环境下，可以用 Persistent Volume (PV) 来保障Milvus/Weaviate的数据持久性。另外，为降低查询延迟，可以在检索服务和LLM服务之间尽量减少网络跳数：例如将向量DB和LLM部署在同一数据中心，或使用内网高速连接。对于高并发请求，开启向量DB的批量查询或并发分片也能提高吞吐。总之，向量数据库是RAG系统的“记忆库”，其选择和优化直接关系到系统的响应速度和可扩展性。

模型选择：检索器、生成器与嵌入向量

检索器模型的选择通常决定了RAG检索阶段的效果。传统有BM25这类基于词频的检索，在精确匹配上表现好，但在语义匹配上不及Dense Vector方案。因此目前主流是使用Dense Embedding模型将查询和文档投射到向量空间，以向量近邻搜索获取相关文档。像Facebook的 DPR（用于原始RAG论文）已不再是唯一选择，近两年出现了许多更强的开源Embedding模型：

E5 模型：由微软等提出的一系列embedding模型。E5通过弱监督对比学习在海量多任务数据上训练，旨在产出通用文本向量表示。据论文报道，E5是第一个在零样本设置下超越经典BM25在BEIR检索基准上的embedding模型。在微调后，E5在MTEB等评测上取得当时最佳成绩，参数效率极高。E5模型家族（base-large等）已开源，实验证明用E5的向量检索能提升RAG准确率，减少LLM幻觉。很多RAG工程开始默认采用E5 embeddings替换原有Sentence-BERT或简单词向量。

GTE 模型：阿里巴巴通义实验室在2023年推出的 General Text Embedding (GTE) 模型，同样引入多阶段对比学习，使用海量多源数据训练。GTE-base仅1.1亿参数，却在Massive Text Embedding Benchmark上超过OpenAI提供的闭源embedding模型（如 text-embedding-ada-002）以及比它大10倍的其它模型。论文指出GTE在OpenAIEmbedding API上取得了优势，甚至无需针对每种编程语言微调，就通过视代码为文本，实现了对代码检索的强性能。这说明像GTE这样精心训练的通用embedding模型，已经能够在质量上媲美甚至超越商用服务，同时提供开源可控的好处。

其他嵌入模型：除了E5、GTE，近期还有如 OpenAI 的 ada-002（虽然闭源但常作为服务调用基线）、BAAI 的 BGE 系列（北京智源推出的通用embedding，在中文和英文上表现突出）、CoCA 系列（Google 提出的跨语言embedding）等。这些模型在MTEB检索榜单上你追我赶。根据2025年中的结果，顶尖的embedding既有开源也有闭源，选型时要考虑性能 vs. 数据安全 vs. 成本等因素。若使用闭源embedding API，如OpenAI或Cohere，能省却模型部署并获得极高性能，但代价是调用费用和潜在延迟。反之开源模型需要自己部署，占用算力但请求成本近乎为零且数据不出内网。

生成器模型方面，即实际负责生成答案的LLM，需要根据应用场景选择。当前RAG可用的生成模型从数亿参数的小模型（适合本地部署回答简易问题），到千亿级的GPT-4、PaLM等。不少开源大模型（如Llama2、Bloomz等）在有外部知识支撑时，也能产出相当不错的答案，这为自托管RAG提供了可能。在工程实践中，有两种常见选择：

使用闭源API：如 OpenAI GPT-3.5/GPT-4, Anthropic Claude 等，优点是模型强大、召回的信息转化为答案的质量高，开发简单（REST API调用）。缺点是成本随调用量线性增长且数据需发给第三方。对于对话类RAG或高要求场景，GPT-4 目前仍是效果标杆，但应权衡成本和延迟。

使用开源LLM：如 Llama2 (Meta开源)、ChatGLM2 (中文)、MPT 等模型，可以部署在自有服务器或云上。开源模型经过微调（Instruction tuning）后也能胜任问答任务，再借助RAG提供的新信息，其答案可靠性可大幅提高。工程上可选用 HuggingFace Transformers 或 vLLM 等推理加速器将这些模型部署为服务。对于企业内部应用，开源模型避免了敏感数据外传的顾虑。需要注意的是，大模型的上下文窗口大小决定了能放入多少检索内容。若上下文有限（如2048 tokens），需要精挑细选最相关的证据；而像Claude或GPT-4有更大窗口（8k-32k tokens）时，可以合并更多段落但也可能引入噪音。因此模型上下文长度影响着RAG的prompt组装策略（详见下节）。

值得一提的是，有些RAG系统还会选用一个辅助的生成模型作为重排器或核查器。例如在检索出的文档基础上，再用一个较小的cross-attention模型或一个精调的LLM对文档相关性进行rerank，以确保喂给主要生成模型的都是高质量证据。这种Retrieval阶段的两级架构在工程上也越来越常见。

总而言之，工程实践中常见组合是：强大的通用embedding模型（如E5/GTE/BGE等）作检索向量生成，高性能的LLM作文本生成，以及必要时的小型rerank模型把关。随着embedding模型能力提高，我们也观察到RAG系统对LLM的依赖有所变化：当检索返回的信息高度相关且完备时，即使使用稍小一些的生成模型也能正确作答。这启示我们在资源有限的情况下，可优先投入提升检索阶段（更好的embedding模型和检索算法），在生成阶段选择“足够好”的模型即可，从而整体上实现成本与效果的平衡。

Prompt 模板设计与上下文构造策略

成功的 RAG 系统不仅取决于检索到正确的信息，还取决于如何将这些信息提供给LLM。Prompt模板设计与上下文构造因此成为工程中的一大考问。以下是最新实践中总结出的关键要点：

明确提示任务，融合检索内容：通常采用的模板是在系统/用户提示中明示模型应利用提供的文档回答问题。例如系统提示可写：“你是一位知识助手，以下是与用户问题相关的资料：...\n请根据以上资料回答用户的问题。” 这样确保模型知道检索文本的权威性，减少臆断。此外，将检索到的各段内容用清晰的格式（如每段前注明“资料1: ...”、“资料2: ...”）放入prompt，可以帮助模型引用和区分不同来源。

控制上下文长度，避免“上下文腐烂 (context rot)”：虽然现代LLM上下文窗口越来越大，但盲目堆砌过多内容可能适得其反。Chroma团队的研究提出，“紧凑、有结构的上下文优于填满整个窗口”。工程中应优选最相关的文段而非贪多，将无关或重复的信息剔除。一项建议是设置一个硬性长度上限（如不超过模型窗口的50%），强制我们筛选内容，从而提高信噪比。实验证明，当上下文里无关信息过多时，模型可能因为“迷失在中间”而忽略真正重要的证据（即“丢失在中部”问题）。因此少而精是良策。

上下文排序和结构：一般将指令或问题放在prompt开头，然后列出检索内容。可以在每段资料后标注来源，鼓励模型引用。例如：“根据资料回答：{用户问题}\n\n资料1【source A】: ...\n资料2【source B】: ...”。这样模型在生成时更倾向于基于资料内容作答。重要的是要保持段落间的逻辑顺序——如果某段提供背景，另一段提供细节，合理的顺序有助于模型理解。避免把主题完全不同的段落混杂。对于多跳问题，也可考虑采用逐步提问的prompt，即引导模型先阅读并提炼一段，然后再处理下一段，类似链式思维。

消除冗余与冲突：在组装上下文时，应检查是否有重复的句子或内容冲突的段落。重复信息既浪费宝贵提示空间，又可能让模型过度重视某些细节。冲突信息（两段内容说法不一致）则可能使模型困惑或产生成分答案。因此实践中常对候选段落进行去重和多样化处理：例如如果两篇文档都是Wiki的相似句子，只保留其一；如果检索结果全来自同一来源，为增加答案全面性，可以挑选不同来源的佐证。Jeff Huber 提出的建议是：“Always re-rank before assemble context”——即重排评分将最有价值的内容放前面，其次才是拼接进入prompt。这有助于模型优先利用排在前部的关键内容。

提示示例与格式：如果任务允许，可以在prompt中加入示范例子让模型参考格式。例如在资料后附加：“<参考示例>用户问: X; 答: (根据资料详细回答)” 等 Few-shot 提示，让模型明白要引经据典地回答。不过要注意这种few-shot会占用部分上下文，要权衡与资料内容的空间分配。另一种技巧是在prompt中要求模型输出引用，例如“请给出答案并引用资料编号”，结合一个格式模板。实践表明，明确要求引用可以减少模型添加幻觉信息的倾向，使其更忠实于检索内容。

动态调整策略：Prompt设计没有万能公式，需要根据实际效果不断迭代。StackOverflow 博客提出可以采用多样性和“中段不丢”策略：确保选入的文段涵盖不同方面，同时不让关键信息淹没在中间。可以尝试改变 top_k 值、chunk 大小，甚至将长段落再切分，以观察对最终答案的影响。如果发现模型输出未充分利用资料，可考虑在prompt中再次强调要求引用资料或将相关句子前移位置。通过引入LLM自身做评估（例如chain-of-thought地检查每段资料的重要性）也是一种新兴思路。

总之，Prompt与上下文构建是RAG工程中需要反复调优的环节。从近期经验来看，一个原则是让模型明确知道提供的资料为何物、如何用：既不能让它忽略资料，也不能让它对资料产生误解。构造一个清晰、有条理的提示（如先提问再列表资料，并注明要求）能极大提升RAG的有效性，使LLM真正发挥“检索增强”的优势而非各说各话。

部署与性能优化策略

将 RAG 系统部署到生产环境，需要在架构和性能上进行周密考虑。当前的最佳实践倾向于基于微服务的模块化架构，结合针对低延迟的优化手段：

1. 微服务架构拆分：典型的生产级RAG系统会拆分为若干独立的服务，分别承担不同职能。例如：

数据接入服务：监控新文档并执行预处理（解析、分块）。可以利用如 Unstructured.io 库或自定义解析器，将文档转为适合索引的文本段落。

嵌入向量服务：提供向量化API，批量接收文本段并返回embedding。通常通过优化的推理服务器（如HuggingFace TGI或TensorRT)部署embedding模型，以充分利用GPU并发。

向量数据库服务：如Milvus/Weaviate/Qdrant容器，负责存储和搜索向量。这一服务通常需要挂载持久存储卷，并在集群中开启副本保证高可用。

RAG 中央API服务：应用层的核心服务，统筹整个RAG流程。它接收用户查询，请求embedding服务将查询编码为向量，在向量DB中检索初步候选，然后可能调用重排模型精排。最后它会调用生成服务得到答案，并将结果返回客户端。该服务通常用轻量的Web框架实现（如FastAPI），以降低自身延迟。

LLM 生成服务：托管一个或多个生成模型的推理端点。可以有多个实例分别负责不同模型（如一个大型LLM生成答案，一个较小模型重排）。采用专门优化的推理后端如 vLLM（支持高并发和动态批处理）或 NVIDIA Triton（支持多模型多并发）是常见选择。这些后端通过maximizing GPU utilization来减少每个请求的平均等待。

观测与日志服务：如Prometheus + Grafana用于指标，ELK或Loki用于日志。完善的监控有助于捕捉瓶颈并及时扩容。

通过这种微服务划分，各组件可以独立扩展和优化：例如查询量增大时，水平扩展API和LLM服务实例；新文档频繁上传时，提高嵌入服务的GPU算力等。这种架构也方便定位问题、模块替换（如想试验新检索算法，只需替换向量服务或API服务中相应逻辑）。

2. 容器化与编排：在生产中，普遍将上述服务容器化，通过 Kubernetes 等编排。K8s提供了容器故障恢复、滚动升级等机制。对于RAG特殊的方面，需注意：

GPU调度：LLM和embedding服务往往需要GPU。可以使用K8s的GPU Operator来确保集群节点正确安装驱动。对需要GPU的Pod设置 nvidia.com/gpu 资源请求，使调度器分配GPU节点。同时可用节点选择和污点策略，将GPU任务和CPU任务隔离调度。

自动扩缩：利用K8s Horizontal Pod Autoscaler，根据指标（如QPS、CPU/GPU利用率）自动增减副本。例如当LLM推理延迟上升或队列积压，可自动扩容LLM服务Pod。在工作负载下降时再缩容，从而在满足低延迟的同时节省资源。

持久存储：向量库、缓存、消息队列等需要PersistVolume，这在K8s中通过PVC实现。需确保数据库重启或迁移后数据仍存在。另外可以考虑主从复制或StatefulSet来提高数据服务的可靠性。

3. 缓存和并行：为了降低平均延迟，可以在架构中引入多级缓存。例如：

对热门查询的检索结果进行缓存，下次直接用缓存的文档而不必查询向量DB。结合embedding的哈希作为键，可以缓存向量相似度查询结果。

对LLM生成的答案也缓存，对于完全相同的问句直接返回上次结果（在知识不频繁变化时有用）。

在LLM推理服务上启用动态批处理功能，将同时到达的多个生成请求合并在一个GPU batch中推理。这可以极大提高GPU利用率，摊薄每条请求的开销。vLLM 等框架在这方面效果明显，已被证明能在保持低延迟的情况下，达到比传统同步模型推理更高的吞吐。

对于序列生成模型，可以使用流式输出技术，让模型一边生成一边将部分结果返回，减少用户感知延迟。

4. 低延迟优化：RAG系统的端到端延迟主要由：检索延迟+LLM生成延迟 构成。降低前者可以：

采用高效索引（如HNSW）和合理的 top_k，使检索查询在几十毫秒内完成。

若使用混合检索(稠密+稀疏)，可并行执行两种检索再合并结果，减少串行等待。

重排阶段若使用小模型，可以部署在CPU以节省GPU，或利用并行线程快速完成。

降低LLM延迟可通过：

选择适当模型规模：在可接受性能前提下，用较小的模型可显著加快推理。

模型优化：使用8-bit/4-bit 量化权重降低计算量；使用蒸馏后的精简模型；或者通过裁剪不必要的中间层。

硬件优化：充分利用GPU并行能力，多卡负载均衡；针对推理的芯片（如Inferentia, Habana等）也可考虑。

预生成：对于可预料的问题（例如每日报告类查询），提前离线生成答案。

可以看到，为达到类似实时问答的体验，需要在架构上分而治之，在实现上尽量并行和缓存，在推理上软硬结合优化。Stack Overflow的一篇博文将生产级RAG引擎称为“高并发、低延迟的服务层”——其背后实际上是一整套工程手段在支撑。综合运用以上策略，已经有团队在实践中实现了面对亿级文档、百并发请求时仍能在亚秒级返回答案的RAG系统。这证明只要架构合理、优化到位，RAG完全可以支撑严苛的生产需求，在保证准确性的同时做到速度快和可扩展。

实践环节：教程与开源资源

构建一个 RAG 系统往往需要理论与实践结合，以下列出一些具体可实践的资源和建议，供工程师学习和搭建：

官方教程与Notebook：Hugging Face 的开源AI食谱中提供了丰富的 RAG 教程。例如 Advanced RAG on HuggingFace documentation using LangChain Notebook演示了如何以 HuggingFace 文档为知识库构建高级RAG系统，并涵盖了检索优化的各个技巧。同系列中还有 RAG with Hugging Face and Milvus、RAG “图书管理员” (Librarian) 实现 with LlamaIndex 等实战案例。这些教程从代码层面详细展示了文档加载、索引构建、查询流程、Prompt设置等步骤，非常适合开发者边看边练。

多语言RAG示例：NAVER 开源了 Bergen 项目，其中包含一个多语言 RAG 基线实现（支持13种语言）。通过该仓库，工程师可以运行端到端的多语言问答系统，并参考其中针对Prompt多语言输出、评估指标调整等细节。对于需要支持非英文内容的RAG，这是一个宝贵的起点。

向量库集成：许多面向开发者的博客和教程介绍了如何将向量数据库接入RAG。如 Medium 上有对比 Pinecone、Weaviate、Milvus、FAISS 等的文章，帮助理解不同方案的优劣。此外，HuggingFace Cookbook 上的 RAG with HF and Milvus 教程展示了用 Python 将 Milvus 作为后端，配合Transformers和LangChain实现问答的全过程。另有 Haystack 官方的教学（如QA at Scale教程）演示如何利用Elasticsearch/FAISS实现向量检索问答。这些资源可以帮助工程师快速上手特定向量库的用法。

开源代码仓库：社区有不少现成的RAG项目可供参考和二次开发。例如：

LangChain 官方模板 中有针对文档QA的链式示例，可直接套用。

LlamaIndex 示例仓库 提供了如何使用不同索引结构和查询路由的代码。

Deepset Haystack 完整的QA管道实现，包含REST API接口和前端UI，便于测试自己的RAG系统。

此外，一些个人项目如 ragatouille、atlas 等也提供了端到端RAG实现，可以对比不同策略。

实验建议（小型可复现RAG系统）：

数据集选择：从小规模公开数据集开始练手，例如 Wikipedia的一些子集，或经典的开放域QA数据集如 Natural Questions、TriviaQA 等。这些都有现成文档-问答对，可以方便评价RAG效果。

构建索引：使用Embedding模型（可选用开源的 intfloat/e5-base 等）对文档集编码，使用FAISS构建向量索引。写一个简易脚本实现：给定问题 -> 计算embedding -> FAISS查询得到前K篇文档文本。

设计Prompt：基于检索到的文档，设计一个回答提示，将问题和文档拼成Prompt喂给一个开源LLM（比如使用7B规模的Llama2-chat）。观察模型输出，并迭代改进Prompt格式，如加上“根据文档回答”等指导语。

评估：手工构造一些测试问答，看模型回答是否引用了文档内容，是否正确。如果有标准答案的数据集，可以借助 EM（完全匹配）、ROUGE 等粗略评估生成答案，与使用原LLM（无检索）输出对比，体会RAG带来的变化。

优化：尝试不同embedding模型（如换用GTE-large）、不同检索参数（top_k大小）、不同LLM（如换成GPT-3.5通过API调用）等，比较答案质量。这有助于理解各组件对最终效果的影响。

通过上述小型实验，开发者可以对 RAG 系统的关键组件有直观认识，并掌握调优的方法。待原型有效后，再逐步扩展到更大数据、更强模型并引入工程化优化。

社区资源：关注 GitHub 上的 Awesome RAG 列表 或 Hugging Face 社区帖子，里面汇集了最新论文、工具和讨论。Stack Overflow 等也有许多关于RAG实践的 Q&A，可帮助解决具体开发中遇到的问题。定期阅读这些资源，有助于了解最新的模型发布、库更新和实践教训。

综上，Retrieval-Augmented Generation 作为连接知识检索与生成的大模型应用技术，在 2025-2026 年迎来了蓬勃发展。从学术到工业，各方面均有突破：一方面新模型和新方法不断提升RAG在复杂推理、多模态、多语种、时序知识等场景下的能力，另一方面完善的开源工具链和工程经验让搭建一个RAG系统变得前所未有地容易。掌握本文梳理的最新进展与方法，并充分利用提供的教程和资源，工程师们就能着手构建起属于自己的检索增强智能问答系统，在实际业务中获得价值。祝您研习愉快，动手实践取得成功！

参考文献：

Aritra Kumar Lahiri, Qinmin Vivian Hu. “AlzheimerRAG: Multimodal Retrieval Augmented Generation for Clinical Use Cases using PubMed articles.” 2025

Hao Yu et al. “Evaluation of Retrieval-Augmented Generation: A Survey.” arXiv preprint 2024

Chanhee Park et al. “MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation.” NAACL 2025

Hongyu Cao et al. “Improving End-to-End Training of Retrieval-Augmented Generation Models via Joint Stochastic Approximation (JSA-RAG).” arXiv preprint 2025

Zehan Li et al. “Towards General Text Embeddings with Multi-stage Contrastive Learning (GTE).” arXiv preprint 2023

Liang Wang et al. “Text Embeddings by Weakly-Supervised Contrastive Pre-training (E5).” arXiv preprint 2022/2024

Swyx, Alessio. “"RAG is Dead, Context Engineering is King" — with Jeff Huber of Chroma.” Latent Space Podcast, Aug 2025

Cameron R. Wolfe. “Practical tips for retrieval-augmented generation (RAG).” Stack Overflow Blog, Aug 2024

LangChain Documentation – “Build a RAG agent with LangChain.” 2023

Hugging Face Cookbook – “Advanced RAG on HuggingFace documentation using LangChain.” 2023

Balázs Fehér. “RAG: An Architectural Review and Strategic Outlook for 2025.” LinkedIn, 2025

DataCamp. “The 7 Best Vector Databases in 2026.” DataCamp Blog, 2026

Hao Liu et al. “HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation.” ACL 2025 (review summary)

Zhuosheng Zhang et al. “RT-RAG: Reasoning in Trees for Retrieval-Augmented Generation in Multi-Hop QA.” arXiv preprint 2026

Simran Arora et al. “Ask Me Anything: A Comprehensive Guide to Retrieval-Augmented Generation.” 2025 (Emergent Mind article)